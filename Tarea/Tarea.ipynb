{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea Apache Spark\n",
    ">Importa las librerías necesarias dónde sea necesario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    ">Crea un SparkSession para comenzar la tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leer CSV\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un DataFrame\n",
    ">Lee el csv datosTarea.csv, mételo a un DF y muéstralo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\Users\\maria\\Documents\\ITAM\\Materias\\7° Semestre\\Fuentes/datosTarea.csv\"\n",
    "\n",
    "#DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro de datos\n",
    ">Consigue todas las empresas que empiecen con 'M' y tengan entre 4000 y 7000 empleados. Sólo muestra los nombres y el número de empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter(df.Name.startswith(\"M\") & (df[\"Number of employees\"] >= 4000) & (df[\"Number of employees\"] <= 7000))\n",
    "\n",
    "# Seleccionar solo las columnas 'Name' y 'Number of employees'\n",
    "selected_columns_df = filtered_df.select(\"Name\", \"Number of employees\")\n",
    "selected_columns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Consigue todos los países que no inicien con las letras 'b', 's' y 'm', pero que tampoco tengan un netword mayor a 500000. Muestra el nombre de la compañía, el país y el networth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "filtered_df = df.filter(~col(\"Country\").startswith(\"B\") &\n",
    "                        ~col(\"Country\").startswith(\"S\") &\n",
    "                        ~col(\"Country\").startswith(\"M\") &\n",
    "                        (col(\"Networth\") <= 500000))\n",
    "\n",
    "selected_columns_df = filtered_df.select(\"Name\", \"Country\", \"Networth\")\n",
    "selected_columns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones\n",
    "Crea una función con @pandas_udf que que le reste a los profits la media en cada renglón. Crea una nueva columna que muestre los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "# pandas_udf\n",
    "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
    "def adjust_profit(profit_series):\n",
    "    return profit_series - profit_series.mean()\n",
    "\n",
    "# DataFrame y una nueva columna\n",
    "adjusted_df = df.withColumn(\"Adjusted Profit\", adjust_profit(df[\"profit\"]))\n",
    "adjusted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping data\n",
    ">Agrupa por industry y muestra cuáles son las empresas con el profit más alto. Muestra los primeros tres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Ventana de partición\n",
    "windowSpec = Window.partitionBy(\"Industry\").orderBy(df[\"profit\"].desc())\n",
    "\n",
    "top3_df = df.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    "    .filter(\"row_number <= 3\") \\\n",
    "    .select(\"Industry\", \"Name\", \"profit\", \"row_number\")\n",
    "top3_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Agrupa por industry y calcula el promedio de empleados que tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_df = df.groupBy(\"Industry\").agg(avg(\"Number of employees\").alias(\"Average Employees\"))\n",
    "average_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    ">Usando Spark SQL, obtén cuántas empresas se fundaron despúes del 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"companies\")\n",
    "\n",
    "# Ejecutar la consulta SQL para contar las empresas fundadas después del 2000\n",
    "result = spark.sql(\"SELECT COUNT(*) as TotalCompanies FROM companies WHERE Founded > 2000\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "result.show()\n",
    "\n",
    "# Detener la SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Regresión Lineal\n",
    ">Con número de empleados, networth y stock price, obtén una predicción del profit a través de una regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"Number of employees\", \"Networth\", \"stock_price\"], outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "final_data = data.select(\"features\", \"profit\")\n",
    "\n",
    "# regresión lineal\n",
    "lr_model = LinearRegression(featuresCol=\"features\", labelCol=\"profit\")\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])\n",
    "trained_model = lr_model.fit(train_data)\n",
    "predictions = trained_model.transform(test_data)\n",
    "predictions.select(\"prediction\", \"profit\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Una vez que obtengas los resultados, a través del api de pandas, conviértelo en un pandas on spark DataFrame y pásalo a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pandas = predictions.to_pandas_on_spark()\n",
    "output_path = \"C:\\Users\\maria\\Documents\\ITAM\\Materias\\7° Semestre\\Fuentes/outputDatosTarea.csv\"\n",
    "predictions_pandas.to_csv(output_path)\n",
    "\n",
    "# Detener la SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
